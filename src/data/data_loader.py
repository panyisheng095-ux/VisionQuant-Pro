import akshare as ak
import pandas as pd
import os
import time
import logging
from collections import OrderedDict
from tqdm import tqdm
from datetime import datetime, timedelta
from typing import Optional

# === è·¯å¾„é…ç½® ===
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(os.path.dirname(CURRENT_DIR))
DATA_RAW_DIR = os.path.join(PROJECT_ROOT, "data", "raw")
DEFAULT_START_DATE = "20100101"

# æ—¥å¿—ï¼ˆä¸å¼ºè¡Œè¦†ç›–å…¨å±€ logging é…ç½®ï¼Œäº¤ç”±å…¥å£å¤„ç»Ÿä¸€é…ç½®ï¼‰
logger = logging.getLogger(__name__)

# å¯¼å…¥æ•°æ®æºé€‚é…å™¨
from .data_source import DataSource, AkshareDataSource
from .jqdata_adapter import JQDataAdapter
from .rqdata_adapter import RQDataAdapter
from .quality_checker import DataQualityChecker


class DataLoader:
    """
    æ•°æ®åŠ è½½å™¨ï¼ˆæ”¯æŒå¤šæ•°æ®æºåˆ‡æ¢ï¼‰
    
    æ”¯æŒçš„æ•°æ®æºï¼š
    - 'akshare': å…è´¹æ•°æ®æºï¼ˆé»˜è®¤ï¼‰
    - 'jqdata': èšå®½æ•°æ®æºï¼ˆéœ€è¦è®¤è¯ï¼‰
    - 'rqdata': ç±³ç­æ•°æ®æºï¼ˆéœ€è¦è®¤è¯ï¼‰
    """
    
    def __init__(self, data_source: str = 'akshare', **kwargs):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        
        Args:
            data_source: æ•°æ®æºåç§° ('akshare', 'jqdata', 'rqdata')
            **kwargs: æ•°æ®æºç‰¹å®šå‚æ•°
                - å¯¹äºjqdata: username, password
                - å¯¹äºrqdata: username, password
        """
        if not os.path.exists(DATA_RAW_DIR):
            os.makedirs(DATA_RAW_DIR)
        self.data_dir = DATA_RAW_DIR
        
        # åˆå§‹åŒ–æ•°æ®æº
        self.data_source_name = data_source
        self.data_source = self._init_data_source(data_source, **kwargs)
        
        # åˆå§‹åŒ–æ•°æ®è´¨é‡æ£€æŸ¥å™¨
        self.quality_checker = DataQualityChecker()
        self.enable_quality_check = kwargs.get('enable_quality_check', True)

        # å†…å­˜çº§ç¼“å­˜ï¼ˆå‡å°‘é‡å¤ç£ç›˜è¯»ï¼‰
        self._mem_cache_enabled = kwargs.get("mem_cache", True)
        self._mem_cache_max = int(kwargs.get("mem_cache_max", 32))
        self._mem_cache = OrderedDict()
    
    def _init_data_source(self, source_name: str, **kwargs) -> DataSource:
        """
        åˆå§‹åŒ–æ•°æ®æº
        
        Args:
            source_name: æ•°æ®æºåç§°
            **kwargs: æ•°æ®æºå‚æ•°
            
        Returns:
            DataSourceå®ä¾‹
        """
        if source_name == 'akshare':
            return AkshareDataSource()
        elif source_name == 'jqdata':
            username = kwargs.get('username') or kwargs.get('jq_username')
            password = kwargs.get('password') or kwargs.get('jq_password')
            return JQDataAdapter(username=username, password=password)
        elif source_name == 'rqdata':
            username = kwargs.get('username') or kwargs.get('rq_username')
            password = kwargs.get('password') or kwargs.get('rq_password')
            return RQDataAdapter(username=username, password=password)
        else:
            logger.warning("æœªçŸ¥æ•°æ®æº: %sï¼Œä½¿ç”¨ akshare ä½œä¸ºé»˜è®¤", source_name)
            return AkshareDataSource()
    
    def switch_data_source(self, source_name: str, **kwargs):
        """
        åˆ‡æ¢æ•°æ®æº
        
        Args:
            source_name: æ–°æ•°æ®æºåç§°
            **kwargs: æ•°æ®æºå‚æ•°
        """
        self.data_source_name = source_name
        self.data_source = self._init_data_source(source_name, **kwargs)
        logger.info("å·²åˆ‡æ¢åˆ°æ•°æ®æº: %s", source_name)
    
    def get_current_data_source(self) -> str:
        """è·å–å½“å‰æ•°æ®æºåç§°"""
        return self.data_source_name

    def get_stock_data(self, symbol, start_date=DEFAULT_START_DATE, end_date=None, adjust="qfq", use_cache=True):
        """
        [æ™ºèƒ½æ›´æ–°ç‰ˆ] è·å–è‚¡ç¥¨æ•°æ®ï¼ˆæ”¯æŒå¤šæ•°æ®æºï¼‰
        
        é€»è¾‘ï¼š
        1. å¦‚æœuse_cache=Trueï¼Œå…ˆæ£€æŸ¥æœ¬åœ°ç¼“å­˜
        2. å¦‚æœæ•°æ®æ»åæˆ–ä¸å­˜åœ¨ï¼Œä»å½“å‰æ•°æ®æºä¸‹è½½
        3. å¦‚æœå½“å‰æ•°æ®æºä¸å¯ç”¨ï¼Œå›é€€åˆ°akshare
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            adjust: å¤æƒç±»å‹
            use_cache: æ˜¯å¦ä½¿ç”¨æœ¬åœ°ç¼“å­˜
        """
        def _to_dt(value, default_dt):
            if value is None or str(value).strip() == "":
                return default_dt
            try:
                dt = pd.to_datetime(value, errors="coerce")
                return default_dt if pd.isna(dt) else dt
            except Exception:
                return default_dt

        req_start_dt = _to_dt(start_date, pd.to_datetime(DEFAULT_START_DATE))
        req_end_dt = _to_dt(end_date, pd.to_datetime(datetime.now().strftime("%Y%m%d")))
        if req_end_dt < req_start_dt:
            req_end_dt = req_start_dt
        start_date = req_start_dt.strftime("%Y%m%d")
        end_date = req_end_dt.strftime("%Y%m%d")

        symbol = str(symbol).strip().zfill(6)
        file_path = os.path.join(self.data_dir, f"{symbol}.csv")

        def _validate_df(df_new):
            if df_new is None or df_new.empty:
                return None
            if self.enable_quality_check:
                quality_result = self.quality_checker.check_data_quality(df_new, symbol)
                if not quality_result['is_valid']:
                    print(f"âš ï¸ [{symbol}] æ•°æ®è´¨é‡æ£€æŸ¥æœªé€šè¿‡ (å¾—åˆ†: {quality_result['score']}/100)")
                    if quality_result['score'] < 50:
                        print(f"  é”™è¯¯: {quality_result['errors']}")
                        return None
            return df_new

        def _fetch_with_fallback(start_dt, end_dt):
            if start_dt > end_dt:
                return pd.DataFrame()
            start_str = start_dt.strftime("%Y%m%d")
            end_str = end_dt.strftime("%Y%m%d")
            df_new = None
            # å½“å‰æ•°æ®æº
            if self.data_source and self.data_source.is_available():
                print(f"â¬‡ï¸ [{self.data_source_name}] æ‹‰å– {symbol} è¡Œæƒ… {start_str}-{end_str}...")
                df_new = self.data_source.get_stock_data(
                    symbol=symbol,
                    start_date=start_str,
                    end_date=end_str,
                    adjust=adjust
                )
                df_new = _validate_df(df_new)
            if (df_new is None or df_new.empty) and self.data_source_name != 'akshare':
                print(f"ğŸ”„ å›é€€åˆ°akshareæ•°æ®æº...")
                fallback_source = AkshareDataSource()
                if fallback_source.is_available():
                    df_new = fallback_source.get_stock_data(
                        symbol=symbol,
                        start_date=start_str,
                        end_date=end_str,
                        adjust=adjust
                    )
                    df_new = _validate_df(df_new)
            return df_new if df_new is not None else pd.DataFrame()

        def _detect_gaps(idx, gap_days: int = 45, max_gaps: int = 10):
            if idx is None or len(idx) < 2:
                return []
            idx = pd.to_datetime(idx)
            idx = idx.sort_values()
            gaps = []
            diffs = idx.to_series().diff().dt.days
            for i, gap in enumerate(diffs):
                if pd.isna(gap):
                    continue
                if gap > gap_days:
                    prev_dt = idx[i - 1]
                    next_dt = idx[i]
                    gap_start = prev_dt + timedelta(days=1)
                    gap_end = next_dt - timedelta(days=1)
                    gaps.append((gap_start, gap_end))
                    if len(gaps) >= max_gaps:
                        break
            return gaps

        # å†…å­˜ç¼“å­˜å‘½ä¸­ï¼ˆä¼˜å…ˆï¼‰
        cache_key = (symbol, start_date, end_date, adjust)
        if use_cache and self._mem_cache_enabled:
            cached = self._mem_cache.get(cache_key)
            if cached is not None:
                return cached.copy()

        need_download = False
        df_cache_all = pd.DataFrame()

        # === 1. æ£€æŸ¥æœ¬åœ°ç¼“å­˜ï¼ˆå¦‚æœå¯ç”¨ï¼‰ ===
        if use_cache and os.path.exists(file_path):
            try:
                df_cache_all = pd.read_csv(file_path, index_col=0, parse_dates=True)
                df_cache_all = self._normalize_columns(df_cache_all)
                df_cache_all = self._ensure_datetime_index(df_cache_all)
                if not df_cache_all.empty:
                    df_cache_all.sort_index(inplace=True)
                    first_date_in_file = pd.to_datetime(df_cache_all.index.min()).normalize()
                    last_date_in_file = pd.to_datetime(df_cache_all.index.max()).normalize()
                    need_earlier = req_start_dt < first_date_in_file
                    need_later = req_end_dt > last_date_in_file
                    need_download = need_earlier or need_later
                    if not need_download:
                        # æ£€æµ‹å¹¶ä¿®å¤å†…éƒ¨å¤§æ®µç¼ºå£ï¼ˆé¿å…ä¸­é€”å¹´ä»½ç©ºçª—ï¼‰
                        gaps = _detect_gaps(df_cache_all.index)
                        if gaps:
                            for g_start, g_end in gaps:
                                if g_end < req_start_dt or g_start > req_end_dt:
                                    continue
                                patch = _fetch_with_fallback(g_start, g_end)
                                if patch is not None and not patch.empty:
                                    patch = self._normalize_columns(patch)
                                    patch = self._ensure_datetime_index(patch)
                                    df_cache_all = pd.concat([df_cache_all, patch], axis=0)
                            df_cache_all = self._ensure_datetime_index(df_cache_all)
                            df_cache_all.sort_index(inplace=True)
                            df_cache_all = df_cache_all[~df_cache_all.index.duplicated(keep='last')]
                            if use_cache:
                                df_cache_all.to_csv(file_path)
                        df_out = df_cache_all.loc[req_start_dt:req_end_dt]
                        if use_cache and self._mem_cache_enabled:
                            self._mem_cache[cache_key] = df_out
                            if len(self._mem_cache) > self._mem_cache_max:
                                self._mem_cache.popitem(last=False)
                        return df_out.copy()
                else:
                    need_download = True
            except Exception as e:
                logger.warning("è¯»å–æœ¬åœ°ç¼“å­˜å¤±è´¥ %s (%s): %s", symbol, file_path, e)
                need_download = True
        else:
            need_download = True

        # === 2. ä»æ•°æ®æºä¸‹è½½ï¼ˆå¦‚æœéœ€è¦ï¼‰ ===
        if need_download:
            if not df_cache_all.empty:
                df_cache_all.sort_index(inplace=True)
                first_date_in_file = pd.to_datetime(df_cache_all.index.min()).normalize()
                last_date_in_file = pd.to_datetime(df_cache_all.index.max()).normalize()
                need_earlier = req_start_dt < first_date_in_file
                need_later = req_end_dt > last_date_in_file

                new_parts = []
                if need_earlier:
                    new_parts.append(_fetch_with_fallback(req_start_dt, first_date_in_file - timedelta(days=1)))
                if need_later:
                    new_parts.append(_fetch_with_fallback(last_date_in_file + timedelta(days=1), req_end_dt))

                for part in new_parts:
                    if part is not None and not part.empty:
                        new_parts_df = self._normalize_columns(part)
                        df_cache_all = pd.concat([df_cache_all, new_parts_df], axis=0)

                if not df_cache_all.empty:
                    df_cache_all = self._ensure_datetime_index(df_cache_all)
                    df_cache_all.sort_index(inplace=True)
                    df_cache_all = df_cache_all[~df_cache_all.index.duplicated(keep='last')]

                    # è¡¥é½ä¸­é—´ç¼ºå£
                    gaps = _detect_gaps(df_cache_all.index)
                    if gaps:
                        for g_start, g_end in gaps:
                            if g_end < req_start_dt or g_start > req_end_dt:
                                continue
                            patch = _fetch_with_fallback(g_start, g_end)
                            if patch is not None and not patch.empty:
                                patch = self._normalize_columns(patch)
                                patch = self._ensure_datetime_index(patch)
                                df_cache_all = pd.concat([df_cache_all, patch], axis=0)
                        df_cache_all = self._ensure_datetime_index(df_cache_all)
                        df_cache_all.sort_index(inplace=True)
                        df_cache_all = df_cache_all[~df_cache_all.index.duplicated(keep='last')]

                    # å¦‚æœä»å­˜åœ¨è¶…å¤§ç¼ºå£ï¼Œå°è¯•å…¨é‡åˆ·æ–°è¯·æ±‚èŒƒå›´
                    gaps_after = _detect_gaps(df_cache_all.index, gap_days=120, max_gaps=1)
                    if gaps_after:
                        full_df = _fetch_with_fallback(req_start_dt, req_end_dt)
                        if full_df is not None and not full_df.empty:
                            full_df = self._normalize_columns(full_df)
                            full_df = self._ensure_datetime_index(full_df)
                            df_cache_all = full_df.copy()

                    if use_cache:
                        df_cache_all.to_csv(file_path)
                    df_out = df_cache_all.loc[req_start_dt:req_end_dt]
                    if use_cache and self._mem_cache_enabled:
                        self._mem_cache[cache_key] = df_out
                        if len(self._mem_cache) > self._mem_cache_max:
                            self._mem_cache.popitem(last=False)
                    return df_out.copy()

                # æ— æ³•è·å–æ–°æ•°æ®æ—¶ï¼Œå›é€€æ—§æ•°æ®
                print(f"âš ï¸ æ‰€æœ‰æ•°æ®æºè·å–å¤±è´¥ï¼Œä½¿ç”¨æœ¬åœ°æ—§æ•°æ®")
                df_out = df_cache_all.loc[req_start_dt:req_end_dt] if not df_cache_all.empty else pd.DataFrame()
                return df_out.copy()

            # æ— æœ¬åœ°ç¼“å­˜ï¼šå…¨é‡æ‹‰å–
            df_new = _fetch_with_fallback(req_start_dt, req_end_dt)
            if df_new is not None and not df_new.empty:
                df_new = self._normalize_columns(df_new)
                df_new = self._ensure_datetime_index(df_new)
                # è¡¥é½ä¸­é—´ç¼ºå£
                gaps = _detect_gaps(df_new.index)
                if gaps:
                    for g_start, g_end in gaps:
                        if g_end < req_start_dt or g_start > req_end_dt:
                            continue
                        patch = _fetch_with_fallback(g_start, g_end)
                        if patch is not None and not patch.empty:
                            patch = self._normalize_columns(patch)
                            patch = self._ensure_datetime_index(patch)
                            df_new = pd.concat([df_new, patch], axis=0)
                    df_new = self._ensure_datetime_index(df_new)
                    df_new.sort_index(inplace=True)
                    df_new = df_new[~df_new.index.duplicated(keep='last')]
                if use_cache:
                    df_new.to_csv(file_path)
                df_out = df_new.loc[req_start_dt:req_end_dt]
                if use_cache and self._mem_cache_enabled:
                    self._mem_cache[cache_key] = df_out
                    if len(self._mem_cache) > self._mem_cache_max:
                        self._mem_cache.popitem(last=False)
                return df_out.copy()

            return pd.DataFrame()

        try:
            if not df_cache_all.empty:
                df_cache_all = self._ensure_datetime_index(df_cache_all)
                return df_cache_all.loc[req_start_dt:req_end_dt].copy()
        except Exception as e:
            logger.warning("æ—¶é—´ç´¢å¼•åˆ‡ç‰‡å¤±è´¥ %s: %s", symbol, e)
        return pd.DataFrame()

    def get_index_data(self, index_code, start_date=DEFAULT_START_DATE, end_date=None, use_cache: bool = False):
        """
        è·å–æŒ‡æ•°OHLCVæ•°æ®ï¼ˆä¼˜å…ˆå½“å‰æ•°æ®æºï¼Œå¤±è´¥å›é€€Akshareï¼‰
        æ³¨æ„ï¼šæ­¤æ¥å£é»˜è®¤ä¸èµ°ç¼“å­˜ï¼Œä»¥ä¿è¯æƒ…ç»ªç±»æŒ‡æ ‡çš„å®æ—¶æ€§ã€‚
        """
        def _to_dt(value, default_dt):
            if value is None or str(value).strip() == "":
                return default_dt
            try:
                dt = pd.to_datetime(value, errors="coerce")
                return default_dt if pd.isna(dt) else dt
            except Exception:
                return default_dt

        req_start_dt = _to_dt(start_date, pd.to_datetime(DEFAULT_START_DATE))
        req_end_dt = _to_dt(end_date, pd.to_datetime(datetime.now().strftime("%Y%m%d")))
        if req_end_dt < req_start_dt:
            req_end_dt = req_start_dt
        start_date = req_start_dt.strftime("%Y%m%d")
        end_date = req_end_dt.strftime("%Y%m%d")

        index_code = str(index_code).strip()
        df = pd.DataFrame()

        # å½“å‰æ•°æ®æº
        if self.data_source and self.data_source.is_available():
            try:
                df = self.data_source.get_index_data(
                    index_code=index_code,
                    start_date=start_date,
                    end_date=end_date
                )
            except Exception as e:
                logger.warning("æŒ‡æ•°æ•°æ®è·å–å¤±è´¥ [%s]: %s", self.data_source_name, e)

        # å›é€€åˆ°Akshare
        if (df is None or df.empty) and self.data_source_name != 'akshare':
            try:
                fallback_source = AkshareDataSource()
                if fallback_source.is_available():
                    df = fallback_source.get_index_data(
                        index_code=index_code,
                        start_date=start_date,
                        end_date=end_date
                    )
            except Exception as e:
                logger.warning("æŒ‡æ•°æ•°æ®å›é€€å¤±è´¥ [akshare]: %s", e)

        if df is None or df.empty:
            return pd.DataFrame()

        df = self._normalize_columns(df)
        df = self._ensure_datetime_index(df)
        return df

    def _ensure_datetime_index(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç¡®ä¿ç´¢å¼•ä¸ºDatetimeIndexï¼Œé¿å…Timestampåˆ‡ç‰‡å¼‚å¸¸"""
        if df is None or df.empty:
            return df
        data = df.copy()
        try:
            if not isinstance(data.index, pd.DatetimeIndex):
                data.index = pd.to_datetime(data.index, errors="coerce")
            data = data[~data.index.isna()]
            data.sort_index(inplace=True)
        except Exception:
            return df
        return data

    def _normalize_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ç»Ÿä¸€å¸¸è§åˆ—åï¼Œä¿è¯ä¸‹æ¸¸é‡ä»·ç‰¹å¾å¯ç¨³å®šè·å–
        """
        if df is None or df.empty:
            return df
        data = df.copy()
        col_map = {}
        for c in data.columns:
            lc = str(c).lower()
            if lc in ["open", "å¼€ç›˜"]:
                col_map[c] = "Open"
            elif lc in ["high", "æœ€é«˜"]:
                col_map[c] = "High"
            elif lc in ["low", "æœ€ä½"]:
                col_map[c] = "Low"
            elif lc in ["close", "æ”¶ç›˜", "æ”¶ç›˜ä»·"]:
                col_map[c] = "Close"
            elif lc in ["volume", "æˆäº¤é‡"]:
                col_map[c] = "Volume"
            elif lc in ["amount", "æˆäº¤é¢", "æˆäº¤é‡‘é¢", "æˆäº¤é¢(å…ƒ)"]:
                col_map[c] = "Amount"
            elif lc in ["turnover", "æ¢æ‰‹ç‡", "æ¢æ‰‹"]:
                col_map[c] = "Turnover"
        if col_map:
            data = data.rename(columns=col_map)
        return data

    def get_top300_stocks(self):
        """è·å–å…¨Aè‚¡åˆ—è¡¨å¹¶æŒ‰å¸‚å€¼æ’åº"""
        # ä¼˜å…ˆä½¿ç”¨å½“å‰æ•°æ®æº
        if self.data_source and self.data_source.is_available():
            try:
                stock_list = self.data_source.get_stock_list()
                if not stock_list.empty:
                    # å¦‚æœæœ‰å¸‚å€¼ä¿¡æ¯ï¼ŒæŒ‰å¸‚å€¼æ’åº
                    if 'market_cap' in stock_list.columns:
                        stock_list = stock_list.sort_values(by='market_cap', ascending=False)
                    return stock_list.head(300)
            except Exception as e:
                print(f"âš ï¸ [{self.data_source_name}] è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
        
        # å›é€€åˆ°akshare
        try:
            df = ak.stock_zh_a_spot_em()
            if 'æ€»å¸‚å€¼' in df.columns:
                df = df.sort_values(by='æ€»å¸‚å€¼', ascending=False)
            df = df.head(300)
            return df[['ä»£ç ', 'åç§°']].rename(columns={'ä»£ç ': 'code', 'åç§°': 'name'})
        except Exception as e:
            print(f"âŒ è·å–åå•å¤±è´¥: {e}")
            return pd.DataFrame()

    def download_batch_data(self, stock_list, start_date=DEFAULT_START_DATE):
        """æ‰¹é‡ä¸‹è½½"""
        print(f"â¬‡ï¸ [æ‰¹é‡ç»´æŠ¤] æ­£åœ¨æ£€æŸ¥å¹¶æ›´æ–° {len(stock_list)} åªè‚¡ç¥¨...")
        for _, row in tqdm(stock_list.iterrows(), total=len(stock_list)):
            symbol = str(row['code']).zfill(6)
            self.get_stock_data(symbol, start_date=start_date)
            # ç¨å¾®å¿«ä¸€ç‚¹ï¼Œå› ä¸ºå¤§éƒ¨åˆ†å¯èƒ½ä¸éœ€è¦ä¸‹è½½
            time.sleep(0.01)


if __name__ == "__main__":
    loader = DataLoader()
    # æµ‹è¯•æ›´æ–°é€»è¾‘
    df = loader.get_stock_data("601899")
    print(f"æœ€æ–°æ•°æ®æ—¥æœŸ: {df.index[-1]}")