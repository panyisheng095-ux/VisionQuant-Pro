import os
import sys
import torch
import torch.nn as nn
from torchvision import transforms
from PIL import Image
import faiss
import pickle
import pandas as pd
import numpy as np
from datetime import datetime

# === 1. åŸºç¡€é…ç½® ===
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
os.environ['OMP_NUM_THREADS'] = '1'

# === 2. è·¯å¾„é…ç½® ===
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(os.path.dirname(CURRENT_DIR))
INDEX_FILE = os.path.join(PROJECT_ROOT, "data", "indices", "cae_faiss.bin")
META_CSV = os.path.join(PROJECT_ROOT, "data", "indices", "meta_data.csv")
META_PKL = os.path.join(PROJECT_ROOT, "data", "indices", "meta.pkl")
MODEL_PATH = os.path.join(PROJECT_ROOT, "data", "models", "cae_best.pth")

if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)
from src.models.autoencoder import QuantCAE


class VisionEngine:
    def __init__(self):
        self.device = torch.device("cpu")
        print(f"ğŸ‘ï¸ [VisionEngine] å¯åŠ¨ä¸­... åŠ è½½æ¨¡å‹: QuantCAE")

        # 1. åŠ è½½æ¨¡å‹
        self.model = QuantCAE().to(self.device)
        if os.path.exists(MODEL_PATH):
            try:
                state_dict = torch.load(MODEL_PATH, map_location=self.device)
                self.model.load_state_dict(state_dict)
                self.model.eval()
            except Exception as e:
                print(f"âŒ æƒé‡åŠ è½½å¤±è´¥: {e}")

        self.pool = nn.AdaptiveAvgPool1d(1024)

        self.preprocess = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
        ])

        self.index = None
        self.meta_data = []

    def reload_index(self):
        if not os.path.exists(INDEX_FILE):
            return False

        print(f"ğŸ“¥ [VisionEngine] åŠ è½½ç´¢å¼•...")
        try:
            self.index = faiss.read_index(INDEX_FILE)
        except Exception as e:
            print(f"âŒ FAISS åŠ è½½å¤±è´¥: {e}")
            return False

        if os.path.exists(META_CSV):
            df = pd.read_csv(META_CSV, dtype=str)
            self.meta_data = df.to_dict('records')
        elif os.path.exists(META_PKL):
            with open(META_PKL, 'rb') as f:
                self.meta_data = pickle.load(f)
        else:
            return False

        print(f"âœ… çŸ¥è¯†åº“å°±ç»ª: {len(self.meta_data)} æ¡è®°å½•")
        return True

    def _image_to_vector(self, img_path):
        try:
            img = Image.open(img_path).convert('RGB')
            input_tensor = self.preprocess(img).unsqueeze(0).to(self.device)
            with torch.no_grad():
                full_feature = self.model.encode(input_tensor)
                reduced_feature = self.pool(full_feature.unsqueeze(1)).squeeze(1)
                return reduced_feature.cpu().numpy().flatten()
        except:
            return None

    def search_similar_patterns(self, target_img_path, top_k=10, query_prices=None):
        """
        æ··åˆæœç´¢ï¼šè§†è§‰ç‰¹å¾ + ä»·æ ¼åºåˆ—ç›¸å…³æ€§
        
        Args:
            target_img_path: æŸ¥è¯¢Kçº¿å›¾è·¯å¾„
            top_k: è¿”å›Top-Kç»“æœ
            query_prices: æŸ¥è¯¢çš„ä»·æ ¼åºåˆ—ï¼ˆ20å¤©æ”¶ç›˜ä»·ï¼‰ï¼Œç”¨äºè®¡ç®—ç›¸å…³æ€§
        """
        if self.index is None:
            if not self.reload_index(): return []

        vec = self._image_to_vector(target_img_path)
        if vec is None: return []

        vec = vec.astype('float32').reshape(1, -1)
        faiss.normalize_L2(vec)

        # === ä¼˜åŒ–1: æ‰©å¤§æœç´¢èŒƒå›´ï¼Œè·å–æ›´å¤šå€™é€‰ ===
        search_k = max(top_k * 10, 200)  # ä»200ä¸ªå€™é€‰ä¸­ç­›é€‰
        D, I = self.index.search(vec, search_k)

        candidates = []
        seen_dates = {}
        ISOLATION_DAYS = 20

        # === ä¼˜åŒ–2: æ··åˆè¯„åˆ† = è§†è§‰ç›¸ä¼¼åº¦ + ä»·æ ¼ç›¸å…³æ€§ ===
        for vector_score, idx in zip(D[0], I[0]):
            if idx >= len(self.meta_data): continue

            info = self.meta_data[idx]
            sym = str(info['symbol']).zfill(6)
            date_str = str(info['date'])

            try:
                current_dt = datetime.strptime(date_str, "%Y%m%d")
            except:
                try:
                    current_dt = datetime.strptime(date_str, "%Y-%m-%d")
                except:
                    continue

            # æ—¶é—´éš”ç¦»æ£€æŸ¥
            is_conflict = False
            if sym in seen_dates:
                for existing_dt in seen_dates[sym]:
                    if abs((current_dt - existing_dt).days) < ISOLATION_DAYS:
                        is_conflict = True
                        break
            if is_conflict:
                continue

            # === ä¼˜åŒ–3: è®¡ç®—ä»·æ ¼åºåˆ—ç›¸å…³æ€§ï¼ˆå¦‚æœæä¾›ï¼‰===
            correlation = 0.0
            if query_prices is not None and len(query_prices) == 20:
                try:
                    # åŠ è½½åŒ¹é…æ¨¡å¼çš„ä»·æ ¼åºåˆ—
                    from src.data.data_loader import DataLoader
                    loader = DataLoader()
                    match_df = loader.get_stock_data(sym)
                    if not match_df.empty:
                        match_df.index = pd.to_datetime(match_df.index)
                        if current_dt in match_df.index:
                            loc = match_df.index.get_loc(current_dt)
                            if loc >= 19:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„å†å²æ•°æ®
                                match_prices = match_df.iloc[loc-19:loc+1]['Close'].values
                                # å½’ä¸€åŒ–ï¼ˆé¿å…ç»å¯¹ä»·æ ¼å·®å¼‚å½±å“ç›¸å…³æ€§ï¼‰
                                query_norm = (query_prices - query_prices.mean()) / (query_prices.std() + 1e-8)
                                match_norm = (match_prices - match_prices.mean()) / (match_prices.std() + 1e-8)
                                # è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°
                                correlation = np.corrcoef(query_norm, match_norm)[0, 1]
                                if np.isnan(correlation):
                                    correlation = 0.0
                except Exception as e:
                    correlation = 0.0

            # === ä¼˜åŒ–4: æ··åˆè¯„åˆ†ï¼ˆè§†è§‰60% + ç›¸å…³æ€§40%ï¼‰===
            # å¦‚æœç›¸å…³æ€§<0.3ï¼Œè¯´æ˜å½¢æ€ç›¸åï¼Œå¤§å¹…é™åˆ†
            if correlation < 0.3:
                final_score = vector_score * 0.3  # å½¢æ€ç›¸åï¼Œå¤§å¹…é™åˆ†
            else:
                # æ­£ç›¸å…³æ—¶ï¼ŒåŠ æƒèåˆ
                final_score = 0.6 * vector_score + 0.4 * correlation

            # === ä¼˜åŒ–5: æé«˜ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œåªä¿ç•™é«˜è´¨é‡åŒ¹é… ===
            if final_score < 0.85:  # æé«˜é˜ˆå€¼ï¼Œè¿‡æ»¤ä½è´¨é‡ç»“æœ
                continue

            candidates.append({
                "symbol": sym,
                "date": date_str,
                "score": float(final_score),
                "vector_score": float(vector_score),
                "correlation": float(correlation)
            })

            seen_dates.setdefault(sym, []).append(current_dt)

        # === ä¼˜åŒ–6: æŒ‰æœ€ç»ˆåˆ†æ•°é‡æ–°æ’åº ===
        candidates.sort(key=lambda x: x['score'], reverse=True)

        # è¿”å›Top-K
        return candidates[:top_k]


if __name__ == "__main__":
    if PROJECT_ROOT not in sys.path: sys.path.insert(0, PROJECT_ROOT)
    v = VisionEngine()
    v.reload_index()
    print("Vision Engine Ready")