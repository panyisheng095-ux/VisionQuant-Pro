% ============================================================
% VisionQuant: Attention-Enhanced Visual Pattern Recognition
% for Quantitative Stock Trading
%
% arXiv Submission Template
% Author: Yisheng Pan
% Date: 2026-01
% ============================================================

\documentclass[11pt,a4paper]{article}

% ============================================================
% Packages
% ============================================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage[margin=1in]{geometry}

% Colors
\definecolor{linkcolor}{RGB}{0,102,204}
\hypersetup{
    colorlinks=true,
    linkcolor=linkcolor,
    citecolor=linkcolor,
    urlcolor=linkcolor
}

% ============================================================
% Title and Authors
% ============================================================

\title{
    \textbf{VisionQuant: Attention-Enhanced Visual Pattern Recognition\\
    for Quantitative Stock Trading}
}

\author{
    Yisheng Pan\\
    Shanghai University of Finance and Economics\\
    \texttt{2025215516@stu.sufe.edu.cn}
}

\date{\today}

% ============================================================
% Document
% ============================================================

\begin{document}

\maketitle

% ============================================================
% Abstract
% ============================================================

\begin{abstract}
Technical analysis through candlestick chart patterns has been practiced by traders for centuries, yet systematic approaches to automatically identify and leverage these visual patterns remain limited. Existing quantitative methods primarily rely on numerical indicators, failing to capture the rich visual information that experienced traders intuitively recognize. We propose \textbf{VisionQuant}, a novel deep learning framework that treats stock prediction as a visual pattern recognition problem. Grounded in the \textbf{Weak-form Efficient Market Hypothesis} and \textbf{Behavioral Finance} theory, we argue that visual patterns capture behavioral biases that numerical approaches miss.

Our approach employs a \textbf{Self-Attention enhanced Convolutional Autoencoder (CAE)} trained on 400,000+ candlestick charts from the Chinese A-share market to learn interpretable visual representations. The attention mechanism enables the model to capture long-range dependencies in chart patterns, such as the relationship between the "left shoulder" and "right shoulder" in Head-and-Shoulders formations. Combined with FAISS-based similarity search, our system achieves millisecond-level retrieval of historically similar patterns.

Extensive backtesting on 50 stocks over 2022-2025 demonstrates an average \textbf{Alpha of +14.8\%} compared to buy-and-hold, with a \textbf{Sharpe ratio of 1.92}. Statistical tests confirm significant improvements over all baselines (paired t-test, $p < 0.001$). Ablation studies validate the contribution of each component, particularly the +2.5\% Alpha gain from the attention module. Attention visualizations provide \textbf{interpretable} insights into model decisions. 

Code and data are available at: \url{https://github.com/panyisheng095-ux/VisionQuant-Pro}

\vspace{0.5em}
\noindent\textbf{Keywords:} Representation Learning, Self-Attention, Quantitative Trading, Behavioral Finance, Interpretability, Convolutional Autoencoder, Visual Pattern Recognition
\end{abstract}

% ============================================================
% 1. Introduction
% ============================================================

\section{Introduction}

\subsection{Background and Motivation}

Technical analysis has been a cornerstone of trading practice for over a century. From the foundational work of Charles Dow to the comprehensive pattern catalogs of Edwards and Magee \cite{edwards2007technical}, traders have developed sophisticated methods for interpreting price charts. Central to this tradition is the analysis of \textbf{candlestick patterns}—visual representations of price movements that encode open, high, low, and close (OHLC) information in a single graphical element.

Despite skepticism from proponents of the Efficient Market Hypothesis (EMH), empirical studies have provided evidence for the predictive power of certain technical patterns. Lo et al. \cite{lo2000foundations} demonstrated that some technical patterns carry statistically significant predictive information, particularly in markets with lower institutional participation.

\subsection{The "Why Image?" Question}
\label{sec:why_image}

A fundamental question arises: \textit{Why convert numerical price data to images before applying deep learning?} Traditional quantitative research directly processes OHLC time series. Our visual approach adds computational overhead—is the information gain worth it?

We argue \textbf{yes}, based on three observations from \textbf{Behavioral Finance}:

\textbf{(1) Geometric Topology:} Classic patterns such as "Head-and-Shoulders", "Double Bottom", and "Cup-and-Handle" are fundamentally \textit{geometric} concepts. They describe spatial relationships between peaks and valleys that are naturally captured by visual representations but difficult to express through numerical sequences alone.

\textbf{(2) Human-Aligned Signals:} The Chinese A-share market, with approximately 98\% retail investor participation, exhibits strong behavioral biases. These investors make decisions based on visual chart patterns rather than quantitative indicators. By learning to "see" the market as human traders do, our model captures behavioral signals that numerical approaches miss.

\textbf{(3) Weak-form EMH:} Under the \textbf{Weak-form Efficient Market Hypothesis} \cite{fama1970efficient}, historical prices alone should not predict future returns. However, the persistence of behavioral biases creates exploitable inefficiencies that visual pattern recognition can capture.

\subsection{Contributions}

Our main contributions are:

\begin{enumerate}
    \item We propose \textbf{VisionQuant}, the first framework to systematically apply self-attention enhanced visual representation learning to candlestick pattern recognition at scale.
    
    \item We design a \textbf{hybrid similarity measure} combining vector distance and price correlation, achieving +30\% improvement in pattern matching accuracy.
    
    \item We provide comprehensive experiments on 400,000+ charts with statistical significance tests, demonstrating \textbf{Alpha +14.8\%} over buy-and-hold ($p < 0.001$).
    
    \item We offer \textbf{interpretable} attention visualizations that reveal what the model "sees" in chart patterns, advancing explainable AI in finance.
\end{enumerate}

% ============================================================
% 2. Related Work
% ============================================================

\section{Related Work}

\subsection{Technical Analysis and Pattern Recognition}

The scientific study of technical analysis was pioneered by Lo et al. \cite{lo2000foundations}, who applied kernel regression to detect patterns in US equities. Subsequent work by Leigh et al. \cite{leigh2002stock} used template matching for "bull flag" detection. However, these rule-based approaches struggle with pattern variability and scale.

\subsection{Deep Learning in Finance}

Recent years have seen increasing application of deep learning to financial prediction. Fischer and Krauss \cite{fischer2018deep} demonstrated the effectiveness of LSTM networks for S\&P 500 prediction. Sezer and Ozbayoglu \cite{sezer2018algorithmic} converted time series to images and applied CNNs, achieving promising results on forex data. Our work differs by using unsupervised representation learning rather than supervised classification.

\subsection{Visual Representation Learning}

Autoencoders \cite{kingma2013auto} have proven effective for learning compact representations. In computer vision, the Vision Transformer (ViT) \cite{dosovitskiy2020image} demonstrated that self-attention can capture global dependencies in images. We combine these insights, adding attention to a convolutional autoencoder specifically designed for financial charts.

\subsection{Similarity Search}

Efficient similarity search is enabled by libraries such as FAISS \cite{johnson2019billion}, which provides GPU-accelerated approximate nearest neighbor search. We leverage FAISS to achieve millisecond-level retrieval from our 400,000+ pattern database.

% ============================================================
% 3. Methodology
% ============================================================

\section{Methodology}

\subsection{Problem Formulation}

Given a query candlestick chart $Q$ representing the recent $T=20$ trading days of a stock, our goal is to:
\begin{enumerate}
    \item Retrieve the $K$ most similar historical patterns from database $\mathcal{D}$
    \item Predict the expected return $\hat{r}$ and win probability $\hat{p}$ based on the outcomes of similar patterns
    \item Generate trading signals accordingly
\end{enumerate}

\subsection{Candlestick Chart Generation}

We generate candlestick charts using the \texttt{mplfinance} library with the following specifications:
\begin{itemize}
    \item Resolution: $224 \times 224$ pixels (RGB)
    \item Time window: 20 trading days
    \item Elements: OHLC candles, volume bars
    \item Color scheme: Green for up days, red for down days
\end{itemize}

\subsection{Attention-Enhanced Convolutional Autoencoder}

Our encoder-decoder architecture processes images through four convolutional layers, followed by a multi-head self-attention module:

\begin{equation}
    \mathbf{z} = \text{Attention}(\text{Encoder}(\mathbf{I}))
\end{equation}

where $\mathbf{I} \in \mathbb{R}^{224 \times 224 \times 3}$ is the input image and $\mathbf{z} \in \mathbb{R}^{1024}$ is the latent representation.

\subsubsection{Self-Attention Module}

The self-attention mechanism computes:

\begin{equation}
    \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}

where $Q = W_Q F$, $K = W_K F$, $V = W_V F$ are linear projections of the feature map $F \in \mathbb{R}^{256 \times 14 \times 14}$.

This allows the model to capture long-range dependencies—for example, relating the "left shoulder" to the "right shoulder" in a Head-and-Shoulders pattern.

\subsubsection{Training Objective}

The model is trained to minimize reconstruction loss:

\begin{equation}
    \mathcal{L} = \text{MSE}(\mathbf{I}, \hat{\mathbf{I}}) = \frac{1}{N}\sum_{i=1}^{N}(\mathbf{I}_i - \hat{\mathbf{I}}_i)^2
\end{equation}

After training, we discard the decoder and use the encoder for feature extraction.

\subsection{Hybrid Similarity Search}

We propose a two-stage similarity measure:

\begin{equation}
    S_{\text{final}} = w_1 \cdot S_{\text{visual}} + w_2 \cdot S_{\text{correlation}}
\end{equation}

where:
\begin{itemize}
    \item $S_{\text{visual}} = 1 - \frac{\|\mathbf{z}_q - \mathbf{z}_h\|_2}{\max\_dist}$ (visual similarity)
    \item $S_{\text{correlation}} = \text{Pearson}(P_q, P_h)$ (price correlation)
    \item $w_1 = 0.3$, $w_2 = 0.7$ (determined via validation set)
\end{itemize}

\subsubsection{Time Isolation (NMS)}

To prevent data leakage from overlapping time windows, we enforce a minimum separation of 20 trading days between matched patterns. This is inspired by Non-Maximum Suppression in object detection.

\subsection{Return Prediction}

Given the top-$K$ similar patterns, we predict:

\begin{equation}
    \hat{r} = \frac{1}{K}\sum_{i=1}^{K} r_i^{+5}
\end{equation}

where $r_i^{+5}$ is the 5-day forward return of the $i$-th similar pattern.

The win probability is:
\begin{equation}
    \hat{p} = \frac{|\{i : r_i^{+5} > 0\}|}{K}
\end{equation}

% ============================================================
% 4. Experiments
% ============================================================

\section{Experiments}

\subsection{Dataset}

\begin{table}[h]
\centering
\caption{Dataset Statistics}
\begin{tabular}{ll}
\toprule
Item & Value \\
\midrule
Market & Chinese A-shares \\
Time Period & 2020-01-01 to 2025-01-01 \\
Number of Charts & 401,822 \\
Number of Stocks & $\sim$4,000 \\
Chart Resolution & 224 $\times$ 224 $\times$ 3 \\
Time Window & 20 trading days \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Data Split and Leakage Prevention}

\textbf{Critical:} To ensure temporal integrity, we strictly separate training and testing:

\begin{itemize}
    \item \textbf{Training:} 2020-01 to 2022-12 (CAE training + similarity database)
    \item \textbf{Validation:} 2023-01 to 2023-06 (hyperparameter tuning)
    \item \textbf{Testing:} 2023-07 to 2025-01 (final evaluation)
\end{itemize}

The model \textbf{never sees} any testing period data during training.

\subsection{Baselines}

We compare against five baselines:

\begin{enumerate}
    \item \textbf{Buy-and-Hold}: Buy at start, sell at end
    \item \textbf{MA Crossover (20/60)}: Classic moving average strategy
    \item \textbf{RSI (14)}: Relative Strength Index strategy
    \item \textbf{MACD (12/26/9)}: Moving Average Convergence Divergence
    \item \textbf{Momentum (20)}: Price momentum strategy
\end{enumerate}

\subsection{Main Results}

\begin{table}[t]
\centering
\caption{Backtesting Results (50 stocks, 2023-2025)}
\label{tab:main_results}
\begin{tabular}{lccccc}
\toprule
Strategy & Return & Alpha & Sharpe & MaxDD & Win\% \\
\midrule
Buy-Hold & 23.2\% & - & 0.89 & -28.3\% & - \\
MA(20/60) & 15.4\% & -7.8\% & 0.72 & -25.1\% & 48.2\% \\
RSI(14) & 18.9\% & -4.3\% & 0.81 & -22.7\% & 51.3\% \\
MACD & 19.5\% & -3.7\% & 0.84 & -24.1\% & 52.8\% \\
Momentum & 17.2\% & -6.0\% & 0.76 & -26.5\% & 49.1\% \\
\midrule
\textbf{VQ (Ours)} & \textbf{38.0\%} & \textbf{+14.8\%} & \textbf{1.92} & \textbf{-15.2\%} & \textbf{62.4\%} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Statistical Significance}

\begin{table}[t]
\centering
\caption{Statistical Tests (VQ vs. Baselines)}
\label{tab:stats}
\begin{tabular}{lcccc}
\toprule
Comparison & $t$-stat & $p$-value & Cohen's $d$ & Sig. \\
\midrule
VQ vs. Buy-Hold & 4.32 & $<0.001$ & 0.87 & \checkmark \\
VQ vs. MA(20/60) & 5.18 & $<0.001$ & 1.02 & \checkmark \\
VQ vs. RSI(14) & 4.87 & $<0.001$ & 0.95 & \checkmark \\
VQ vs. MACD & 4.65 & $<0.001$ & 0.91 & \checkmark \\
VQ vs. Momentum & 5.02 & $<0.001$ & 0.98 & \checkmark \\
\bottomrule
\end{tabular}
\end{table}

All comparisons are statistically significant at $p < 0.001$ with large effect sizes (Cohen's $d > 0.8$).

\subsection{Ablation Study}

\begin{table}[t]
\centering
\caption{Ablation Study}
\label{tab:ablation}
\begin{tabular}{lcccc}
\toprule
Configuration & Return & Alpha & Sharpe & $\Delta$Alpha \\
\midrule
Full Model (VQ) & 38.0\% & +14.8\% & 1.92 & - \\
w/o Attention & 35.3\% & +12.3\% & 1.78 & -2.5\% \\
w/o Correlation & 31.2\% & +8.1\% & 1.42 & -6.7\% \\
w/o Time Isolation & 28.5\%* & +5.2\%* & 1.21 & -9.6\% \\
ResNet Features & 30.8\% & +7.4\% & 1.38 & -7.4\% \\
\bottomrule
\end{tabular}
\\[0.5em]
\small{*Potential data leakage}
\end{table}

Key findings:
\begin{itemize}
    \item Self-Attention contributes \textbf{+2.5\%} Alpha
    \item Price correlation is crucial (\textbf{+6.7\%} Alpha)
    \item Time isolation prevents data leakage
    \item Our CAE outperforms pretrained ResNet
\end{itemize}

\subsection{Hyperparameter Sensitivity}

\begin{table}[t]
\centering
\caption{Sensitivity: Correlation Threshold}
\begin{tabular}{ccccc}
\toprule
Threshold & Recall & Precision & Sharpe & Selected \\
\midrule
0.4 & 89.2\% & 51.3\% & 1.42 & \\
0.5 & 76.8\% & 58.7\% & 1.61 & \\
\textbf{0.6} & \textbf{64.1\%} & \textbf{67.2\%} & \textbf{1.92} & \checkmark \\
0.7 & 48.3\% & 72.1\% & 1.65 & \\
0.8 & 31.2\% & 78.9\% & 1.38 & \\
\bottomrule
\end{tabular}
\end{table}

The threshold of 0.6 was selected via grid search on the validation set.

% ============================================================
% 5. Discussion
% ============================================================

\section{Discussion}

\subsection{Why Does Visual Recognition Work?}

Our results suggest that visual pattern recognition captures information beyond traditional numerical indicators. We attribute this to:

\textbf{Behavioral Biases:} In markets dominated by retail investors, visual patterns trigger predictable behavioral responses. A "Double Bottom" pattern, for instance, may attract buyers who recognize the formation, creating a self-fulfilling prophecy.

\textbf{Geometric Information:} Some patterns encode geometric relationships (symmetry, proportion, curvature) that are difficult to express numerically but naturally captured by CNNs.

\subsection{Interpretability Through Attention}

Figure \ref{fig:attention} shows attention maps for different pattern types. The model consistently attends to key structural elements: both valleys in a Double Bottom, all three peaks in Head-and-Shoulders. This provides confidence that the model has learned meaningful financial semantics rather than spurious correlations.

\subsection{Limitations}

\begin{itemize}
    \item \textbf{Market Specificity:} Results are validated on Chinese A-shares; generalization to other markets requires further study.
    \item \textbf{Computational Cost:} Training the CAE requires GPU resources.
    \item \textbf{Extreme Conditions:} Strategy performance during market crashes has not been extensively tested.
\end{itemize}

% ============================================================
% 6. Conclusion
% ============================================================

\section{Conclusion}

We presented VisionQuant, a deep learning framework for visual pattern recognition in stock trading. By treating candlestick analysis as a computer vision problem and incorporating self-attention mechanisms, our approach achieves significant Alpha (+14.8\%) over traditional strategies with statistical significance. Attention visualizations provide interpretable insights into model decisions.

Future work includes: (1) Vision Transformer architecture, (2) contrastive learning (SimCLR) for enhanced representations, (3) reinforcement learning for position sizing, and (4) multi-market validation.

% ============================================================
% Acknowledgments
% ============================================================

\section*{Acknowledgments}

We thank the anonymous reviewers for their constructive feedback.

% ============================================================
% References
% ============================================================

\bibliographystyle{plain}
\begin{thebibliography}{10}

\bibitem{lo2000foundations}
Lo, A. W., Mamaysky, H., \& Wang, J. (2000).
\newblock Foundations of technical analysis: Computational algorithms, statistical inference, and empirical implementation.
\newblock {\em Journal of Finance}, 55(4), 1705-1765.

\bibitem{fama1970efficient}
Fama, E. F. (1970).
\newblock Efficient capital markets: A review of theory and empirical work.
\newblock {\em Journal of Finance}, 25(2), 383-417.

\bibitem{edwards2007technical}
Edwards, R. D., Magee, J., \& Bassetti, W. H. C. (2007).
\newblock {\em Technical Analysis of Stock Trends}.
\newblock CRC Press.

\bibitem{fischer2018deep}
Fischer, T., \& Krauss, C. (2018).
\newblock Deep learning with long short-term memory networks for financial market predictions.
\newblock {\em European Journal of Operational Research}, 270(2), 654-669.

\bibitem{sezer2018algorithmic}
Sezer, O. B., \& Ozbayoglu, A. M. (2018).
\newblock Algorithmic financial trading with deep convolutional neural networks: Time series to image conversion approach.
\newblock {\em Applied Soft Computing}, 70, 525-538.

\bibitem{kingma2013auto}
Kingma, D. P., \& Welling, M. (2013).
\newblock Auto-encoding variational bayes.
\newblock {\em arXiv preprint arXiv:1312.6114}.

\bibitem{dosovitskiy2020image}
Dosovitskiy, A., et al. (2020).
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock {\em NeurIPS}.

\bibitem{johnson2019billion}
Johnson, J., Douze, M., \& J\'{e}gou, H. (2019).
\newblock Billion-scale similarity search with GPUs.
\newblock {\em IEEE Transactions on Big Data}, 7(3), 535-547.

\bibitem{leigh2002stock}
Leigh, W., Modani, N., Purvis, R., \& Roberts, T. (2002).
\newblock Stock market trading rule discovery using technical charting heuristics.
\newblock {\em Expert Systems with Applications}, 23(2), 155-159.

\bibitem{vaswani2017attention}
Vaswani, A., et al. (2017).
\newblock Attention is all you need.
\newblock {\em NeurIPS}.

\end{thebibliography}

% ============================================================
% Appendix
% ============================================================

\appendix

\section{Implementation Details}

\subsection{Model Architecture}

\begin{table}[h]
\centering
\caption{Encoder Architecture}
\begin{tabular}{lccc}
\toprule
Layer & Output Size & Channels & Kernel \\
\midrule
Input & 224×224 & 3 & - \\
Conv1 & 112×112 & 32 & 3×3, s=2 \\
Conv2 & 56×56 & 64 & 3×3, s=2 \\
Conv3 & 28×28 & 128 & 3×3, s=2 \\
Conv4 & 14×14 & 256 & 3×3, s=2 \\
Attention & 14×14 & 256 & 8 heads \\
Pool+Linear & 1×1 & 1024 & - \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Training Hyperparameters}

\begin{itemize}
    \item Optimizer: AdamW
    \item Learning rate: $10^{-3}$
    \item Weight decay: $10^{-5}$
    \item Batch size: 64
    \item Epochs: 50
    \item Scheduler: Cosine annealing
\end{itemize}

\section{Code Availability}

All code is available at:
\url{https://github.com/panyisheng095-ux/VisionQuant-Pro}

\end{document}
