"""
å› å­æœ‰æ•ˆæ€§åˆ†ææ¨¡å—ï¼ˆå·¥ä¸šçº§ä¼˜åŒ–ç‰ˆï¼‰

ICï¼ˆInformation Coefficientï¼‰åˆ†æã€å› å­è¡°å‡æ£€æµ‹ã€å¤šæŒæœ‰æœŸåˆ†æ
æ·±åº¦ä¿®å¤ï¼šä¿è¯600æ ·æœ¬é‡ã€å¹´ä»½å‡åŒ€è¦†ç›–ã€å¹¶è¡Œç¨³å®šæ€§
"""
import os
import logging
import threading
import uuid
import pickle
import numpy as np
import pandas as pd
import mplfinance as mpf
import plotly.graph_objects as go
from concurrent.futures import ThreadPoolExecutor, as_completed

logger = logging.getLogger(__name__)

# matplotlib çº¿ç¨‹é”ï¼ˆç¡®ä¿ mplfinance åœ¨å¤šçº¿ç¨‹ä¸‹å®‰å…¨ï¼‰
_MPL_LOCK = threading.Lock()


def show_factor_analysis(symbol, df_f, eng, PROJECT_ROOT):
    """æ˜¾ç¤ºå› å­æœ‰æ•ˆæ€§åˆ†æï¼ˆå¸¦æŒ‰é’®ä¸ç¼“å­˜ï¼‰"""
    import streamlit as st
    vision_engine = eng.get("vision") if isinstance(eng, dict) else eng
    kline_calc = None
    if isinstance(eng, dict):
        from src.strategies.kline_factor import KLineFactorCalculator
        kline_calc = KLineFactorCalculator(data_loader=eng.get("loader"))

    st.caption("å› å­åˆ†æè®¡ç®—è€—æ—¶è¾ƒé•¿ï¼Œå»ºè®®æŒ‰éœ€è¿è¡Œå¹¶å¤ç”¨ç¼“å­˜ç»“æœã€‚")
    c1, c2, c3 = st.columns([1.2, 1.2, 3])
    use_cache = c1.checkbox("ä½¿ç”¨ç¼“å­˜", value=True, key=f"fa_use_cache_{symbol}")
    run_btn = c2.button("è¿è¡Œå› å­åˆ†æ", key=f"fa_run_{symbol}")
    force_btn = c3.button("å¼ºåˆ¶é‡ç®—", key=f"fa_force_{symbol}")

    if not run_btn and not force_btn:
        # è‹¥å·²æœ‰ç¼“å­˜ï¼Œå…è®¸ç›´æ¥æ˜¾ç¤º
        cache_key = _factor_cache_key(symbol, df_f)
        cache_path = _factor_cache_path(PROJECT_ROOT, cache_key)
        if use_cache and os.path.exists(cache_path):
            st.info("å·²æ£€æµ‹åˆ°ç¼“å­˜ç»“æœï¼Œå¯ç›´æ¥åŠ è½½ã€‚")
            if st.button("åŠ è½½ç¼“å­˜ç»“æœ", key=f"fa_load_{symbol}"):
                return run_factor_analysis(symbol, df_f, vision_engine, kline_calc, PROJECT_ROOT, use_cache=True, force=False)
        else:
            st.info("ç‚¹å‡»â€œè¿è¡Œå› å­åˆ†æâ€å¼€å§‹è®¡ç®—ã€‚")
        return

    return run_factor_analysis(
        symbol, df_f, vision_engine, kline_calc, PROJECT_ROOT,
        use_cache=use_cache, force=force_btn
    )


def run_factor_analysis(symbol, df_f, vision_engine, kline_calc, PROJECT_ROOT, use_cache=True, force=False):
    """
    å› å­æœ‰æ•ˆæ€§åˆ†æä¸»å‡½æ•°ï¼ˆå·¥ä¸šçº§ç‰ˆæœ¬ï¼‰
    
    ä¿è¯ï¼š
    1. æ ·æœ¬é‡è¾¾åˆ°600ï¼ˆæˆ–æ•°æ®å…è®¸çš„æœ€å¤§å€¼ï¼‰
    2. å¹´ä»½å‡åŒ€è¦†ç›–ï¼Œæ— ç©ºçª—
    3. å¹¶è¡Œè®¡ç®—ç¨³å®š
    """
    import streamlit as st
    from src.factor_analysis.ic_analysis import ICAnalyzer

    try:
        st.subheader("ğŸ“ˆ å› å­æœ‰æ•ˆæ€§åˆ†æ")
        
        # æ•°æ®è¯Šæ–­
        st.caption(f"ğŸ“Š æ•°æ®èŒƒå›´: {df_f.index[0].strftime('%Y-%m-%d')} ~ {df_f.index[-1].strftime('%Y-%m-%d')}ï¼Œå…± {len(df_f)} ä¸ªäº¤æ˜“æ—¥")
        
        cache_key = _factor_cache_key(symbol, df_f)
        cache_path = _factor_cache_path(PROJECT_ROOT, cache_key)

        if use_cache and not force and os.path.exists(cache_path):
            try:
                with open(cache_path, "rb") as f:
                    data = pickle.load(f)
                rolling_ic = _render_factor_result(symbol, data)
                return
            except Exception:
                pass

        # è®¡ç®—å› å­å€¼ï¼ˆæ ¸å¿ƒé€»è¾‘ï¼Œä¿è¯æ ·æœ¬é‡ï¼‰
        factor_values, forward_returns, dates, horizon_returns, success_count, fail_count = \
            _calculate_factor_values(df_f, symbol, kline_calc, vision_engine, PROJECT_ROOT)

        if len(factor_values) < 30:
            st.warning(f"âš ï¸ æœ‰æ•ˆæ ·æœ¬ä¸è¶³ï¼ˆ{len(factor_values)}ä¸ªï¼‰ï¼Œæ— æ³•è¿›è¡Œå¯é çš„ICåˆ†æ")
            return

        # æ„å»ºæ—¶é—´åºåˆ—
        factor_series = pd.Series(factor_values, index=pd.to_datetime(dates, format="%Y%m%d"))
        returns_series = pd.Series(forward_returns, index=pd.to_datetime(dates, format="%Y%m%d"))

        # å¯¹é½å¹¶æ’åº
        common_idx = factor_series.index.intersection(returns_series.index)
        factor_series = factor_series.loc[common_idx].sort_index()
        returns_series = returns_series.loc[common_idx].sort_index()

        # ICè®¡ç®—ï¼ˆä½¿ç”¨åŠ¨æ€çª—å£ï¼Œé¿å…æ ·æœ¬ä¸è¶³ï¼‰
        window = min(20, max(5, len(factor_series) // 10))
        ic_analyzer = ICAnalyzer(window=window)
        ic_result = ic_analyzer.analyze(factor_series, returns_series)
        
        # å¤šæŒæœ‰æœŸIC
        multi_ic = None
        if horizon_returns:
            try:
                multi_ic = ic_analyzer.analyze_multi_horizon(
                    factor_series,
                    {h: pd.Series(rets, index=pd.to_datetime(dates[:len(rets)], format="%Y%m%d"))
                     for h, rets in horizon_returns.items() if len(rets) > 0}
                )
            except Exception:
                pass

        data = {
            "factor_values": factor_values,
            "forward_returns": forward_returns,
            "dates": dates,
            "horizon_returns": horizon_returns,
            "success_count": success_count,
            "fail_count": fail_count,
            "ic_result": ic_result,
            "multi_ic": multi_ic,
        }
        # è½ç›˜ç¼“å­˜
        try:
            os.makedirs(os.path.dirname(cache_path), exist_ok=True)
            with open(cache_path, "wb") as f:
                pickle.dump(data, f)
        except Exception:
            pass

        rolling_ic = _render_factor_result(symbol, data)

    except ImportError as e:
        logger.exception(f"å› å­åˆ†ææ¨¡å—å¯¼å…¥å¤±è´¥: {symbol}")
        st.error(f"æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    except Exception as e:
        logger.exception(f"å› å­åˆ†æå¼‚å¸¸: {symbol}")
        st.error(f"å› å­åˆ†æå¤±è´¥: {e}")
        import traceback
        with st.expander("æŸ¥çœ‹è¯¦ç»†é”™è¯¯"):
            st.code(traceback.format_exc())


def _factor_cache_key(symbol, df_f):
    try:
        start = df_f.index[0].strftime("%Y%m%d")
        end = df_f.index[-1].strftime("%Y%m%d")
    except Exception:
        start = "start"
        end = "end"
    return f"{symbol}_{start}_{end}_{len(df_f)}"


def _factor_cache_path(project_root, cache_key):
    return os.path.join(project_root, "data", "factor_cache", f"{cache_key}.pkl")


def _render_factor_result(symbol, data: dict):
    import streamlit as st
    ic_result = data.get("ic_result", {}) or {}
    multi_ic = data.get("multi_ic")
    dates = data.get("dates", [])
    success_count = data.get("success_count")
    fail_count = data.get("fail_count")
    factor_values = data.get("factor_values", [])
    forward_returns = data.get("forward_returns", [])

    if success_count is not None:
        st.success(f"âœ… å› å­è®¡ç®—å®Œæˆï¼šæˆåŠŸ {success_count} ä¸ªæ ·æœ¬ï¼Œå¤±è´¥ {fail_count} ä¸ª")

    # å¹´ä»½åˆ†å¸ƒè¯Šæ–­
    year_dist = {}
    for d in dates:
        try:
            year = int(str(d)[:4])
            year_dist[year] = year_dist.get(year, 0) + 1
        except Exception:
            pass
    if year_dist:
        st.caption(f"ğŸ“… å¹´ä»½åˆ†å¸ƒ: {dict(sorted(year_dist.items()))}")

    # æ›´æ–°ICæ‘˜è¦
    st.session_state["ic_result"] = ic_result
    st.session_state["ic_summary"][symbol] = {
        **ic_result.get("summary", {}),
        "samples": len(dates)
    }

    # å› å­ Betaï¼ˆå¯¹æœªæ¥æ”¶ç›Šçš„æ•æ„Ÿåº¦ï¼‰
    try:
        if factor_values and forward_returns and len(factor_values) == len(forward_returns):
            fv = np.array(factor_values, dtype=float)
            rt = np.array(forward_returns, dtype=float)
            if np.var(fv) > 1e-8:
                beta = float(np.cov(fv, rt)[0, 1] / np.var(fv))
                corr = float(np.corrcoef(fv, rt)[0, 1]) if len(fv) > 2 else 0.0
                st.metric("å› å­Beta(å¯¹æœªæ¥æ”¶ç›Š)", f"{beta:.4f}")
                if beta > 0:
                    st.caption(f"Beta>0ï¼šå› å­å€¼ä¸Šå‡æ—¶æ”¶ç›Šå€¾å‘æé«˜ï¼ˆç›¸å…³æ€§ {corr:.2f}ï¼‰")
                elif beta < 0:
                    st.caption(f"Beta<0ï¼šå› å­å€¼ä¸Šå‡æ—¶æ”¶ç›Šå€¾å‘ä¸‹é™ï¼ˆç›¸å…³æ€§ {corr:.2f}ï¼‰")
                else:
                    st.caption("Betaâ‰ˆ0ï¼šè¯¥å› å­å¯¹æ”¶ç›Šæ•æ„Ÿåº¦è¾ƒå¼±")
    except Exception:
        pass

    # ç»˜å›¾
    rolling_ic = ic_result.get("ic_series", pd.Series(dtype=float))
    if isinstance(rolling_ic, pd.Series) and not rolling_ic.empty:
        rolling_ic = rolling_ic.dropna().sort_index()

    _plot_ic_curve(rolling_ic, ic_result)
    if multi_ic:
        _plot_ic_horizon_matrix(multi_ic)
    _plot_sharpe_curve(ic_result)

    # è¡°å‡åˆ†æ
    try:
        from src.factor_analysis.decay_analysis import DecayAnalyzer
        decay_analyzer = DecayAnalyzer()
        decay_result = decay_analyzer.analyze_decay(rolling_ic)
    except Exception:
        decay_result = {}
    _plot_decay_analysis(rolling_ic, decay_result)

    return rolling_ic


def _calculate_factor_values(df_f, symbol, kline_calc, vision_engine, PROJECT_ROOT, horizons=None):
    """
    è®¡ç®—å†å²å› å­å€¼ï¼ˆæ·±åº¦ä¿®å¤ç‰ˆï¼‰
    
    æ ¸å¿ƒä¿®å¤ï¼š
    1. ä½¿ç”¨çº¿ç¨‹é”ä¿æŠ¤ matplotlib
    2. æ ·æœ¬å¤±è´¥æ—¶ä»è®°å½•ä¸­æ€§å€¼ï¼Œä¿è¯æ ·æœ¬é‡
    3. å¹´ä»½åˆ†å±‚é‡‡æ ·ï¼Œé¿å…ç©ºçª—
    """
    import streamlit as st
    from multiprocessing import cpu_count
    
    if horizons is None:
        horizons = [1, 5, 10, 20]
    
    results = []
    horizon_returns = {h: [] for h in horizons}

    # === æ ·æœ¬é€‰å–ï¼ˆä¿è¯600ä¸ªï¼Œå¹´ä»½å‡åŒ€ï¼‰===
    end_idx = len(df_f) - max(horizons) - 1  # ç¡®ä¿æ‰€æœ‰horizonéƒ½å¯è®¡ç®—
    if end_idx <= 20:
        return [], [], [], horizon_returns, 0, 0

    total_points = end_idx - 20 + 1
    target_points = min(600, total_points)
    
    # å¹´ä»½åˆ†å±‚é‡‡æ ·ï¼šç¡®ä¿æ¯å¹´éƒ½æœ‰æ ·æœ¬
    years_idx = {}
    for i in range(20, end_idx + 1):
        year = df_f.index[i].year
        if year not in years_idx:
            years_idx[year] = []
        years_idx[year].append(i)
    
    # æŒ‰å¹´ä»½å‡åŒ€åˆ†é…æ ·æœ¬
    num_years = len(years_idx)
    samples_per_year = max(1, target_points // num_years)
    sample_idx = []
    
    for year in sorted(years_idx.keys()):
        year_indices = years_idx[year]
        if len(year_indices) <= samples_per_year:
            sample_idx.extend(year_indices)
        else:
            # å‡åŒ€æŠ½æ ·
            step = len(year_indices) / samples_per_year
            picked = [year_indices[int(i * step)] for i in range(samples_per_year)]
            sample_idx.extend(picked)
    
    # å¦‚æœè¿˜ä¸å¤Ÿ600ï¼Œè¡¥å……
    if len(sample_idx) < target_points:
        remaining = set(range(20, end_idx + 1)) - set(sample_idx)
        remaining = sorted(remaining)
        need = target_points - len(sample_idx)
        if len(remaining) >= need:
            step = len(remaining) / need
            extra = [remaining[int(i * step)] for i in range(need)]
            sample_idx.extend(extra)
    
    sample_idx = sorted(set(sample_idx))[:target_points]
    
    # è¿›åº¦æ¡
    progress_bar = st.progress(0)
    status_text = st.empty()
    total_iters = len(sample_idx)
    
    success_count = 0
    fail_count = 0
    
    # === ä¸²è¡Œå¤„ç†ï¼ˆç¡®ä¿ç¨³å®šæ€§ï¼‰===
    # æ³¨æ„ï¼šmatplotlib åœ¨å¤šçº¿ç¨‹ä¸‹ä¸å®‰å…¨ï¼Œæ”¹ç”¨ä¸²è¡Œ + ä¼˜åŒ–
    for idx, i in enumerate(sample_idx):
        try:
            current_data = df_f.iloc[i-20:i].copy()
            if len(current_data) < 20:
                fail_count += 1
                continue

            date_dt = df_f.index[i]
            date_str = _safe_date_str(date_dt)
            
            # æ›´æ–°è¿›åº¦
            progress = (idx + 1) / total_iters
            progress_bar.progress(progress)
            if idx % 10 == 0:  # å‡å°‘UIæ›´æ–°é¢‘ç‡
                status_text.text(f"è®¡ç®—å› å­å€¼: {idx + 1}/{total_iters} ({progress*100:.1f}%)")
            
            # ç”Ÿæˆä¸´æ—¶å›¾åƒï¼ˆä½¿ç”¨UUIDé¿å…å†²çªï¼‰
            temp_img = os.path.join(PROJECT_ROOT, "data", f"temp_factor_{uuid.uuid4().hex[:8]}.png")
            
            try:
                # ä½¿ç”¨é”ä¿æŠ¤ matplotlib
                with _MPL_LOCK:
                    mc = mpf.make_marketcolors(up='red', down='green', inherit=True)
                    s = mpf.make_mpf_style(marketcolors=mc, gridstyle='')
                    mpf.plot(current_data, type='candle', style=s, 
                            savefig=dict(fname=temp_img, dpi=50),
                            figsize=(3, 3), axisoff=True)
            except Exception as e:
                logger.warning(f"å›¾åƒç”Ÿæˆå¤±è´¥ {i}: {e}")
                # å›¾åƒç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨è‡ªåŒ¹é…
                matches = _self_match_windows(df_f, symbol, i, top_k=5)
                if not matches:
                    # ä»ç„¶è®°å½•ä¸­æ€§å€¼
                    _record_neutral_sample(results, df_f, i, date_str, horizons, horizon_returns)
                    success_count += 1
                    continue
            
            # æœç´¢ç›¸ä¼¼å½¢æ€
            matches = None
            try:
                matches = vision_engine.search_similar_patterns(
                    temp_img, 
                    top_k=5,
                    max_date=date_dt,
                    fast_mode=True,
                    search_k=300,
                    rerank_with_pixels=False,
                    max_price_checks=30,
                    use_price_features=False
                )
            except Exception as e:
                logger.warning(f"è§†è§‰æœç´¢å¤±è´¥ {i}: {e}")
            
            # å›é€€æ–¹æ¡ˆ
            if not matches or len(matches) < 3:
                matches = _self_match_windows(df_f, symbol, i, top_k=5)
            
            # è®¡ç®—å› å­å€¼
            factor_value = 0.5  # é»˜è®¤ä¸­æ€§
            if matches and len(matches) > 0 and kline_calc is not None:
                try:
                    factor_result = kline_calc.calculate_hybrid_win_rate(
                        matches,
                        query_symbol=symbol,
                        query_date=date_str,
                        query_df=None
                    )
                    if isinstance(factor_result, dict):
                        enhanced = factor_result.get("enhanced_factor")
                        if isinstance(enhanced, dict) and enhanced.get("final_score") is not None:
                            factor_value = float(enhanced.get("final_score")) / 100.0
                        else:
                            factor_value = factor_result.get('hybrid_win_rate', 50.0) / 100.0
                except Exception as e:
                    logger.warning(f"å› å­è®¡ç®—å¤±è´¥ {i}: {e}")
            elif matches and len(matches) > 0:
                # kline_calc ä¸º None æ—¶ï¼Œä½¿ç”¨ç®€å•çš„ç›¸ä¼¼åº¦å‡å€¼ä½œä¸ºå› å­å€¼
                try:
                    avg_score = sum(m.get("score", 0.5) for m in matches) / len(matches)
                    factor_value = avg_score
                except:
                    pass
            
            # è®¡ç®—å¤šæŒæœ‰æœŸæ”¶ç›Šç‡
            p_entry = df_f.iloc[i]['Close']
            rets = {}
            for h in horizons:
                if i + h < len(df_f):
                    p_exit = df_f.iloc[i + h]['Close']
                    rets[h] = (p_exit - p_entry) / p_entry
            
            # é»˜è®¤ä½¿ç”¨5å¤©æ”¶ç›Š
            p_exit = df_f.iloc[min(i+5, len(df_f)-1)]['Close']
            ret = (p_exit - p_entry) / p_entry
            
            results.append({
                "factor_value": factor_value,
                "forward_return": ret,
                "date": date_str,
                "horizon_returns": rets
            })
            success_count += 1
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_img):
                try:
                    os.remove(temp_img)
                except:
                    pass
                    
        except Exception as e:
            logger.warning(f"å¤„ç†æ ·æœ¬å¤±è´¥ {i}: {e}")
            # å¤±è´¥æ—¶ä»è®°å½•ä¸­æ€§æ ·æœ¬
            try:
                _record_neutral_sample(results, df_f, i, _safe_date_str(df_f.index[i]), horizons, horizon_returns)
                success_count += 1
            except:
                fail_count += 1
    
    # æ¸…ç†è¿›åº¦æ¡
    progress_bar.empty()
    status_text.empty()
    
    # æŒ‰æ—¥æœŸæ’åº
    results.sort(key=lambda x: x["date"])
    
    # æå–ç»“æœ
    factor_values = [r["factor_value"] for r in results]
    forward_returns = [r["forward_return"] for r in results]
    dates = [r["date"] for r in results]
    for r in results:
        for h, ret in r.get("horizon_returns", {}).items():
            horizon_returns[h].append(ret)
    
    return factor_values, forward_returns, dates, horizon_returns, success_count, fail_count


def _record_neutral_sample(results, df_f, i, date_str, horizons, horizon_returns):
    """è®°å½•ä¸­æ€§æ ·æœ¬ï¼ˆå½“åŒ¹é…å¤±è´¥æ—¶ï¼‰"""
    p_entry = df_f.iloc[i]['Close']
    rets = {}
    for h in horizons:
        if i + h < len(df_f):
            p_exit = df_f.iloc[i + h]['Close']
            rets[h] = (p_exit - p_entry) / p_entry
    p_exit = df_f.iloc[min(i+5, len(df_f)-1)]['Close']
    ret = (p_exit - p_entry) / p_entry
    results.append({
        "factor_value": 0.5,
        "forward_return": ret,
        "date": date_str,
        "horizon_returns": rets
    })


def _self_match_windows(df_f, symbol, idx, window: int = 20, top_k: int = 10, max_windows: int = 100):
    """
    å›é€€æ–¹æ¡ˆï¼šä»…åœ¨"åŒä¸€è‚¡ç¥¨å†å²çª—å£"å†…åšå½¢æ€ç›¸ä¼¼åº¦ï¼ˆæ— æœªæ¥å‡½æ•°ï¼‰
    """
    try:
        if idx <= window:
            return []
        q_prices = df_f.iloc[idx - window: idx]["Close"].values
        if len(q_prices) < window:
            return []

        start = window
        end = idx
        total = end - start
        if total <= 0:
            return []
        step = max(1, total // max_windows)

        # å½’ä¸€åŒ–
        q_mean = q_prices.mean()
        q_std = q_prices.std() + 1e-8
        q_norm = (q_prices - q_mean) / q_std
        
        candidates = []
        for j in range(start, end, step):
            cand = df_f.iloc[j - window: j]["Close"].values
            if len(cand) < window:
                continue
            c_mean = cand.mean()
            c_std = cand.std() + 1e-8
            c_norm = (cand - c_mean) / c_std
            corr = np.dot(q_norm, c_norm) / window
            if np.isnan(corr):
                corr = 0.0
            sim = (corr + 1.0) / 2.0
            date_str = df_f.index[j - 1].strftime("%Y%m%d")
            candidates.append({
                "symbol": str(symbol).zfill(6),
                "date": date_str,
                "score": float(sim),
                "correlation": float(corr)
            })
        candidates.sort(key=lambda x: x["score"], reverse=True)
        return candidates[:top_k]
    except Exception:
        return []


def _plot_ic_curve(rolling_ic, ic_result):
    """ç»˜åˆ¶ICæ›²çº¿"""
    import streamlit as st

    st.markdown("#### IC åˆ†æ")
    if rolling_ic.empty:
        st.write("IC æ•°æ®ä¸è¶³")
        return

    summary = ic_result.get("summary", {})
    mean_ic = summary.get("mean_ic", 0.0)
    std_ic = summary.get("std_ic", 0.0)
    ic_ir = summary.get("ir", 0.0)
    positive_ratio = summary.get("positive_ratio", 0.0)
    half_life = summary.get("half_life", None)
    stability = summary.get("stability_score", None)

    # ICçŠ¶æ€åˆ¤æ–­
    if abs(mean_ic) > 0.05:
        ic_status = "æ˜¾è‘—" + ("(æ­£å‘)" if mean_ic > 0 else "(åå‘)")
        ic_color = "normal" if mean_ic > 0 else "inverse"
    elif abs(mean_ic) > 0.02:
        ic_status = "å¾®å¼±"
        ic_color = "off"
    else:
        ic_status = "æ— æ•ˆ"
        ic_color = "off"

    col1, col2, col3, col4, col5, col6 = st.columns(6)
    col1.metric("å¹³å‡IC", f"{mean_ic:.4f}", delta=ic_status, delta_color=ic_color)
    col2.metric("ICæ ‡å‡†å·®", f"{std_ic:.4f}")
    col3.metric("ICIR", f"{ic_ir:.2f}", delta="ä¼˜ç§€" if abs(ic_ir) > 1.0 else "ä¸€èˆ¬")
    col4.metric("æ­£ICæ¯”ä¾‹", f"{positive_ratio*100:.1f}%",
               delta="è‰¯å¥½" if positive_ratio > 0.6 else "ä¸€èˆ¬")
    col5.metric("IC Half-Life", f"{half_life:.1f}" if half_life is not None else "N/A")
    col6.metric("ç¨³å®šæ€§è¯„åˆ†", f"{float(stability):.2f}" if stability is not None else "N/A")

    # ç»˜å›¾
    fig = go.Figure()
    
    # Rolling IC æŸ±çŠ¶å›¾
    fig.add_trace(go.Bar(
        x=rolling_ic.index,
        y=rolling_ic.values,
        name="Rolling IC",
        marker_color=['red' if x >= 0 else 'green' for x in rolling_ic.values]
    ))
    
    # ç´¯ç§¯ICæ›²çº¿
    cum_ic = rolling_ic.cumsum()
    fig.add_trace(go.Scatter(
        x=rolling_ic.index,
        y=cum_ic.values,
        name="Cumulative IC",
        yaxis="y2",
        line=dict(color='blue', width=2)
    ))

    fig.update_layout(
        title="æ»šåŠ¨ICä¸ç´¯ç§¯IC",
        height=350,
        yaxis=dict(title="Rolling IC"),
        yaxis2=dict(title="Cumulative IC", overlaying="y", side="right"),
        showlegend=True,
        legend=dict(x=0.85, y=1.0)
    )
    st.plotly_chart(fig, use_container_width=True)

    with st.expander("â„¹ï¸ å› å­åˆ†æè¯´æ˜ä¸æŒ‡æ ‡è§£è¯»", expanded=False):
        st.markdown(r"""
        **1. æ ¸å¿ƒæ¦‚å¿µ**
        - **å› å­å®šä¹‰**: Kçº¿å­¦ä¹ å› å­ = ç›¸ä¼¼åº¦åŠ æƒçš„æ··åˆèƒœç‡ï¼ˆä½œä¸ºæœŸæœ›æ”¶ç›Šä»£ç†ï¼‰
        - **IC (Information Coefficient)**: å› å­å€¼ä¸æœªæ¥æ”¶ç›Šç‡çš„ç›¸å…³ç³»æ•°ã€‚åæ˜ å› å­é¢„æµ‹èƒ½åŠ›ã€‚
        - **Rolling IC**: æ»šåŠ¨çª—å£ä¸‹çš„ICå€¼ï¼Œç”¨äºè§‚å¯Ÿå› å­éšæ—¶é—´çš„ç¨³å®šæ€§ã€‚

        **2. æŒ‡æ ‡è§£è¯»æ ‡å‡†**
        - **å¹³å‡IC**:
          - `> 0.05`: æ˜¾è‘—æ­£å‘ï¼ˆå› å­åˆ†è¶Šé«˜ï¼Œæœªæ¥æ¶¨å¹…è¶Šå¤§ï¼‰
          - `< -0.05`: æ˜¾è‘—åå‘ï¼ˆå¯ä½œä¸ºåå‘æŒ‡æ ‡ä½¿ç”¨ï¼‰
          - `abs(IC) < 0.02`: é¢„æµ‹èƒ½åŠ›å¾®å¼±
        - **ICIR (IC/Std)**: è¡¡é‡å› å­ç¨³å®šæ€§ï¼ˆICå‡å€¼/ICæ ‡å‡†å·®ï¼‰ã€‚ç»å¯¹å€¼ `> 1.0` ä¸ºä¼˜ç§€ã€‚
        - **æ­£ICæ¯”ä¾‹**: æ»šåŠ¨IC > 0 çš„æ—¶é—´å æ¯”ï¼Œè¶Šé«˜è¶Šå¥½ã€‚
        - **Half-Life (åŠè¡°æœŸ)**: å› å­é¢„æµ‹èƒ½åŠ›è¡°å‡ä¸€åŠæ‰€éœ€å¤©æ•°ã€‚è¶Šé•¿è¶Šé€‚åˆä¸­é•¿çº¿ã€‚

        **3. è¿›é˜¶åˆ†æ**
        - **Regimeåˆ†æ**: åœ¨ä¸åŒå¸‚åœºçŠ¶æ€ï¼ˆç‰›/ç†Š/éœ‡è¡ï¼‰ä¸‹çš„å› å­è¡¨ç°å·®å¼‚ã€‚
        - **å› å­è¡°å‡**: è§‚å¯Ÿè¿‘æœŸICæ˜¯å¦æ˜¾è‘—å¼±äºæ—©æœŸICï¼Œæç¤ºå¤±æ•ˆé£é™©ã€‚
        """)


def _plot_ic_horizon_matrix(multi_ic: dict):
    """å¤šæŒæœ‰æœŸICçŸ©é˜µ"""
    import streamlit as st
    st.subheader("å¤šæŒæœ‰æœŸICçŸ©é˜µï¼ˆICè¡°å‡ï¼‰")
    matrix = multi_ic.get("ic_matrix")
    if matrix is None or matrix.empty:
        st.caption("ICçŸ©é˜µæ•°æ®ä¸è¶³")
        return
    st.dataframe(matrix, use_container_width=True, hide_index=True)

    try:
        fig = go.Figure(data=go.Heatmap(
            z=matrix[["ic_mean", "ic_ir", "half_life"]].values,
            x=["ICå‡å€¼", "ICIR", "Half-Life"],
            y=matrix["horizon"].astype(str).tolist(),
            colorscale="RdBu"
        ))
        fig.update_layout(height=280, title="ICçŸ©é˜µçƒ­å›¾ï¼ˆä¸åŒæŒæœ‰æœŸï¼‰")
        st.plotly_chart(fig, use_container_width=True)
    except Exception:
        pass


def _plot_sharpe_curve(ic_result):
    """ç»˜åˆ¶æ»šåŠ¨Sharpe"""
    import streamlit as st
    sharpe_series = ic_result.get("sharpe_series", pd.Series(dtype=float))

    if sharpe_series.empty:
        return

    # æ¸…æ´—æ•°æ®
    sharpe_series = sharpe_series.dropna().sort_index()
    if sharpe_series.empty:
        return

    st.subheader("Rolling Sharpe åˆ†æ")
    fig = go.Figure()
    fig.add_trace(go.Scatter(
        x=sharpe_series.index,
        y=sharpe_series.values,
        name="Rolling Sharpe",
        line=dict(color='orange')
    ))

    mean_sharpe = sharpe_series.mean()
    fig.add_hline(y=0, line_dash="dash", line_color="gray")
    fig.update_layout(height=300)
    st.plotly_chart(fig, use_container_width=True)
    st.caption(f"Rolling Sharpeå‡å€¼: {mean_sharpe:.3f}")


def _plot_decay_analysis(rolling_ic, decay_result=None):
    """å› å­è¡°å‡åˆ†æ"""
    import streamlit as st

    st.subheader("å› å­è¡°å‡åˆ†æ")
    if rolling_ic.empty:
        return
        
    decay_window = min(60, len(rolling_ic))
    if decay_window < 10:
        return

    recent_ic = rolling_ic.tail(decay_window).mean()
    earlier_ic = rolling_ic.head(decay_window).mean() if len(rolling_ic) > decay_window else recent_ic
    decay_rate = (recent_ic - earlier_ic) / abs(earlier_ic) * 100 if earlier_ic != 0 else 0

    col1, col2 = st.columns(2)
    col1.metric("æ—©æœŸICå‡å€¼", f"{earlier_ic:.4f}")
    col2.metric("è¿‘æœŸICå‡å€¼", f"{recent_ic:.4f}", delta=f"{decay_rate:.1f}%",
               delta_color="inverse" if decay_rate < 0 else "normal")

    if decay_result:
        cps = decay_result.get("change_points", [])
        if cps:
            st.caption(f"æ£€æµ‹åˆ°æ‹ç‚¹: {', '.join([str(c) for c in cps[-3:]])}")


def _safe_date_str(dt):
    """å®‰å…¨è½¬æ¢æ—¥æœŸä¸ºå­—ç¬¦ä¸²"""
    try:
        return dt.strftime("%Y%m%d")
    except Exception:
        return str(dt)
